# Stock Market Data Pipeline with Apache Airflow

A production-ready, Dockerized data pipeline that automatically fetches stock market data from Alpha Vantage API and stores it in PostgreSQL using Apache Airflow for orchestration.

## üéØ Features

- **Automated Data Collection**: Hourly scheduled fetching of stock market data
- **Robust Error Handling**: Comprehensive retry logic and graceful failure management
- **Scalable Architecture**: Docker-based deployment with connection pooling
- **Data Quality Validation**: Automated data quality checks after each run
- **Production Ready**: Environment-based configuration and security best practices
- **Monitoring**: Detailed logging and execution statistics via Airflow UI

## üöÄ Quick Start

### 1. Clone and Navigate

```bash
cd d:\adarsh\assignment
```

### 2. Configure Environment Variables

Copy the example environment file and update with your credentials:

```bash
# On Windows PowerShell
Copy-Item .env.example .env
```

Edit `.env` file and update the following:

```env
# REQUIRED: Get your free API key from https://www.alphavantage.co/support/#api-key
ALPHA_VANTAGE_API_KEY=YOUR_ACTUAL_API_KEY_HERE

# Optional: Customize stock symbols (comma-separated)
STOCK_SYMBOLS=AAPL,GOOGL,MSFT,TSLA,AMZN

# Optional: Update Airflow credentials (set your own secure password)
AIRFLOW_USERNAME=admin
AIRFLOW_PASSWORD=<your_secure_password_here>
```

### 3. Build and Start the Pipeline

```bash
docker-compose up -d
```

This command will:
- Pull necessary Docker images
- Create PostgreSQL database with stock_data table
- Initialize Airflow database
- Create Airflow admin user
- Start Airflow webserver and scheduler

### 4. Access Airflow UI

1. Open browser and navigate to: `http://localhost:8080`
2. Login with credentials from `.env` file (default: `admin` / `admin123`)
3. Find the DAG: `stock_market_data_pipeline`
4. Toggle the DAG to "ON" to enable automatic scheduling

### 5. Trigger Manual Run (Optional)

Click the "Play" button (‚ñ∂Ô∏è) on the DAG to trigger an immediate run.

## üìÅ Project Structure

```
assignment/
‚îú‚îÄ‚îÄ docker-compose.yml          # Docker orchestration configuration
‚îú‚îÄ‚îÄ .env                        # Environment variables (DO NOT COMMIT)
‚îú‚îÄ‚îÄ .env.example               # Example environment configuration
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îú‚îÄ‚îÄ README.md                  # This file
‚îÇ
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ stock_market_dag.py    # Airflow DAG definition
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ fetch_stock_data.py    # Stock data fetching logic
‚îÇ
‚îú‚îÄ‚îÄ init_db/
‚îÇ   ‚îî‚îÄ‚îÄ init.sql              # Database initialization script
‚îÇ
‚îú‚îÄ‚îÄ logs/                      # Airflow logs (auto-generated)
‚îî‚îÄ‚îÄ plugins/                   # Airflow plugins (optional)
```

## üîß Configuration

### Environment Variables

| Variable | Description | Default | Required |
|----------|-------------|---------|----------|
| `ALPHA_VANTAGE_API_KEY` | Alpha Vantage API key | - | ‚úÖ Yes |
| `STOCK_SYMBOLS` | Comma-separated stock symbols | AAPL,GOOGL,MSFT,TSLA,AMZN | ‚ùå No |
| `POSTGRES_USER` | PostgreSQL username | airflow | ‚ùå No |
| `POSTGRES_PASSWORD` | PostgreSQL password | airflow_password_123 | ‚ùå No |
| `POSTGRES_DB` | PostgreSQL database name | airflow | ‚ùå No |
| `AIRFLOW_USERNAME` | Airflow web UI username | admin | ‚ùå No |
| `AIRFLOW_PASSWORD` | Airflow web UI password | admin123 | ‚ùå No |

### Pipeline Schedule

- **Default**: Hourly (`@hourly`)
- **Modify**: Edit `schedule_interval` in `dags/stock_market_dag.py`

Available options:
- `@hourly` - Every hour
- `@daily` - Every day at midnight
- `0 */6 * * *` - Every 6 hours
- `0 9 * * 1-5` - Weekdays at 9 AM

### Stock Symbols

Add or modify symbols in `.env`:

```env
STOCK_SYMBOLS=AAPL,GOOGL,MSFT,TSLA,AMZN,NVDA,META,NFLX
```

## üìä Database Schema

### `stock_data` Table

| Column | Type | Description |
|--------|------|-------------|
| `id` | SERIAL | Primary key |
| `symbol` | VARCHAR(10) | Stock ticker symbol |
| `timestamp` | TIMESTAMP | Data point timestamp |
| `open_price` | NUMERIC(12,4) | Opening price |
| `high_price` | NUMERIC(12,4) | Highest price |
| `low_price` | NUMERIC(12,4) | Lowest price |
| `close_price` | NUMERIC(12,4) | Closing price |
| `volume` | BIGINT | Trading volume |
| `created_at` | TIMESTAMP | Record creation time |
| `updated_at` | TIMESTAMP | Last update time |

### Views

- `latest_stock_prices`: Latest price for each symbol
- `daily_stock_stats`: Daily aggregated statistics

## üîç Monitoring and Logs

### Airflow UI

- **URL**: http://localhost:8080
- **Features**:
  - View DAG runs and task status
  - Access logs for each task
  - Monitor execution times
  - View XCom variables (execution statistics)

### Docker Logs

View real-time logs:

```bash
# All services
docker-compose logs -f

# Specific service
docker-compose logs -f airflow-scheduler
docker-compose logs -f airflow-webserver
docker-compose logs -f postgres
```

## üóÉÔ∏è Database Access

### Using psql (PostgreSQL CLI)

```bash
docker exec -it stock_pipeline_postgres psql -U airflow -d airflow
```

### Sample Queries

```sql
-- View latest stock prices
SELECT * FROM latest_stock_prices;

-- Count records per symbol
SELECT symbol, COUNT(*) as record_count 
FROM stock_data 
GROUP BY symbol 
ORDER BY record_count DESC;

-- View recent records
SELECT * FROM stock_data 
ORDER BY timestamp DESC 
LIMIT 10;

-- Daily statistics
SELECT * FROM daily_stock_stats 
WHERE symbol = 'AAPL' 
ORDER BY trade_date DESC 
LIMIT 7;
```

## üõ†Ô∏è Maintenance

### Stop the Pipeline

```bash
docker-compose down
```

### Stop and Remove All Data

```bash
docker-compose down -v
```

### Restart Services

```bash
docker-compose restart
```

### Update Dependencies

1. Modify `requirements.txt`
2. Rebuild containers:

```bash
docker-compose down
docker-compose up -d --build
```

### View Service Status

```bash
docker-compose ps
```

## üêõ Troubleshooting

### Issue: "API Rate Limit Exceeded"

**Cause**: Alpha Vantage free tier limits: 5 calls/minute, 500 calls/day

**Solution**:
- Reduce number of symbols in `STOCK_SYMBOLS`
- Increase schedule interval (e.g., from hourly to every 4 hours)
- Upgrade to paid Alpha Vantage plan

### Issue: "Database Connection Failed"

**Solution**:
```bash
# Check if PostgreSQL is running
docker-compose ps postgres

# View PostgreSQL logs
docker-compose logs postgres

# Restart PostgreSQL
docker-compose restart postgres
```

### Issue: "DAG Not Appearing in Airflow UI"

**Solution**:
```bash
# Check scheduler logs
docker-compose logs airflow-scheduler

# Verify DAG file syntax
docker exec airflow_scheduler python /opt/airflow/dags/stock_market_dag.py

# Restart scheduler
docker-compose restart airflow-scheduler
```

### Issue: "Permission Denied" on Windows

**Solution**:
```bash
# Run PowerShell as Administrator
# Check Docker Desktop is running
# Ensure drives are shared in Docker Desktop settings
```

## üìà Scaling Considerations

### Increase Symbols

The pipeline includes rate limiting (12 seconds between calls) to respect API limits:

```python
# In fetch_stock_data.py
time.sleep(12)  # Adjust if needed
```

### Database Performance

For large datasets, consider:
- Partitioning `stock_data` table by date
- Archiving old data
- Adding additional indexes

### Airflow Executor

For production, consider upgrading from LocalExecutor to:
- **CeleryExecutor**: Distributed task execution
- **KubernetesExecutor**: Dynamic scaling on Kubernetes

## üîê Security Best Practices

1. **Never commit `.env` file** to version control
2. **Use strong passwords** in production
3. **Rotate API keys** regularly
4. **Enable Airflow authentication** (already configured)
5. **Use secrets management** for production (AWS Secrets Manager, HashiCorp Vault)

## üìù API Rate Limits

### Alpha Vantage Free Tier

- **5 API calls per minute**
- **500 API calls per day**

### Pipeline Rate Limiting

The pipeline automatically waits 12 seconds between API calls to stay within rate limits:

- 5 symbols = ~1 minute per run
- 10 symbols = ~2 minutes per run

## üß™ Testing

### Test API Connection

```bash
docker exec airflow_scheduler python -c "
import os
import requests
api_key = os.getenv('ALPHA_VANTAGE_API_KEY')
response = requests.get(f'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=AAPL&interval=60min&apikey={api_key}')
print(response.json())
"
```

### Test Database Connection

```bash
docker exec airflow_scheduler python -c "
import psycopg2
import os
conn = psycopg2.connect(
    host='postgres',
    dbname=os.getenv('STOCK_DB_NAME'),
    user=os.getenv('STOCK_DB_USER'),
    password=os.getenv('STOCK_DB_PASSWORD')
)
print('Database connection successful!')
conn.close()
"
```

### Run Script Manually

```bash
docker exec airflow_scheduler python /opt/airflow/scripts/fetch_stock_data.py
```

## üì¶ Deliverables Checklist

- ‚úÖ `docker-compose.yml` - Docker Compose configuration
- ‚úÖ `dags/stock_market_dag.py` - Airflow DAG with scheduling
- ‚úÖ `scripts/fetch_stock_data.py` - Data fetching script
- ‚úÖ `init_db/init.sql` - Database schema initialization
- ‚úÖ `requirements.txt` - Python dependencies
- ‚úÖ `.env.example` - Environment variable template
- ‚úÖ `README.md` - Comprehensive documentation

## üéì Assignment Compliance

### Requirements Met

| Requirement | Implementation | Status |
|------------|----------------|--------|
| Docker Compose deployment | `docker-compose.yml` with multi-service setup | ‚úÖ |
| Airflow orchestration | DAG with 5 tasks, hourly schedule | ‚úÖ |
| API data fetching | Alpha Vantage integration with retry logic | ‚úÖ |
| JSON parsing | Comprehensive parsing with validation | ‚úÖ |
| PostgreSQL storage | Table with indexes and views | ‚úÖ |
| Error handling | Try-except blocks, retry logic, graceful failures | ‚úÖ |
| Environment variables | All credentials in .env file | ‚úÖ |
| Scalability | Connection pooling, rate limiting, modular design | ‚úÖ |
| Code quality | Well-documented, type hints, logging | ‚úÖ |

## üìû Support

For issues or questions:

1. Check the [Troubleshooting](#-troubleshooting) section
2. Review Airflow logs: `docker-compose logs airflow-scheduler`
3. Check database logs: `docker-compose logs postgres`

## üìÑ License

This project is created for educational purposes as part of a data engineering assignment.

## üôè Acknowledgments

- [Alpha Vantage](https://www.alphavantage.co/) for free stock market API
- [Apache Airflow](https://airflow.apache.org/) for workflow orchestration
- [PostgreSQL](https://www.postgresql.org/) for reliable data storage

---

